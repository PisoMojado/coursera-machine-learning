{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1\n",
    "\n",
    "The solution for the efficient selection of hypothesis parameters (at least in linear regression, and other forms of machine learning) is called gradient descent.\n",
    "\n",
    "Gradient descent essentially amounts to starting your guesses for parameters at some location in the vector space for the parameters, determine the best direction to move around in, and take a step in that direction, and repeating our steps and choice of direction until we arrive at a minimum location.\n",
    "\n",
    "One hazard to performing gradient descent is that you can arrive at local minima based simply on where you start in your descent, and how large your steps are.\n",
    "\n",
    "*Note* := is the assignment operator during the lecture\n",
    "*Note* alpha in our gradient descent models our \"learning rate\", or the rate in which we take steps as we descend the gradient.\n",
    "\n",
    "An important note that Andrew brings up is that the gradient descent formalism assumes that you simultaneously update all theta terms in one round of gradient descent. (This is trivial, and is simply a truth in how you can't update values of a parameter while you're still updating other parameters in later/concurrent steps).\n",
    "\n",
    "Gradient descent algorithm: $\\theta_j := \\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)$ Repeat until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2\n",
    "\n",
    "Whoops, I got pretty interested in this presentation. Andrew mostly just simulated the gradient descent algorithm, and demonstrated a few key takeaways:\n",
    "1. A learning rate that is defined to be too small will result in a very lengthy operation\n",
    "2. A learning rate that is defined too large may actually fail to converge on a local/global minimum, and perhaps even diverge. This is due to an effect, where each step in gradient descent over-shoots a minimum and, in circumstances where the points are \"higher\" in the local bell shape, will get amplified by an even higher tangent slope, and continue to climb out of the bell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3\n",
    "\n",
    "Andrew begins by explaining how we perform a partial derivation on our cost function in one step of our gradient descent.\n",
    "\n",
    "Andrew skips the actual act of derivation.\n",
    "\n",
    "### Big Point\n",
    "Andrew reveals that, in linear regression, it turns out that the cost function **always** produces a convex function, and so there is only one minimum in the entire space.\n",
    "\n",
    "The version of gradient descent discussed so far is referred to commonly as \"Batch\" Gradient Descent. The reason for this is that, in each step, we're looking at the entire set of training examples to determine cost. There are other techniques for gradient descent which do not do this, and we'll learn about those in the future.\n",
    "\n",
    "Conclusion: If you have taken linear algebra before, you may already be aware that there is in fact a numerical solution for linear regression (the normal equations method), and so the iterative (and computationally expensive) gradient descent is not strictly necessary, but this will be covered later. Even still, it turns out that gradient descent has been shown to outperform the normal equations method in very large data sets (presumably with a non-batched version of gradient descent)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
