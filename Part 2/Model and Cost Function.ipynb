{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function Lecture Notes\n",
    "\n",
    "## Lecture 1\n",
    "\n",
    "In the first example, Andrew arranged a linear regression problem.\n",
    "The training set is a set of data that shows houses by their size in feet<sup>2</sup> as x, and their prices in 1m USD as y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = [[2104, 460], [1416, 232], [1534, 315], [852, 178]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider, then, that our training set can be viewed as a series of two-dimensional vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2104  460]\n",
      " [1416  232]\n",
      " [1534  315]\n",
      " [ 852  178]]\n"
     ]
    }
   ],
   "source": [
    "print(np.matrix(training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are seeking the predict the price of homes by their square footage, then we'd expect our hypothesis function to appear as a simple two-paramter linear function:\n",
    "\\begin{align}\n",
    "h_\\theta(x) & = \\theta_0 + \\theta_1x\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we want to choose (or discover) the values of θ<sub>0</sub> and θ<sub>1</sub> such that h<sub>θ</sub>(x) is close to the value of y in our training examples (x,y)\n",
    "\n",
    "Formally, we are going to find values for our parameters that minimize the value of J(θ<sub>0</sub>,θ<sub>1</sub>), where\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta_0,\\theta_1) & = \\frac{1}{2m}\\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right)^2\n",
    "\\end{align}\n",
    "Note: we are minimizing half of the squared difference because it makes the summation a little easier to work with later, and it is held as a given that minimizing by one half should yield the same ultimate values of θ<sub>0</sub> and θ<sub>1</sub>\n",
    "\n",
    "J(θ<sub>0</sub>,θ<sub>1</sub>) is what is called a **cost function**, and you can think of it as a way to measure the \"cost\" of choosing two particular parameters for your hypothesis function. Parameters which produce a (relatively) large value of J are bad, because - we can see this from how J is defined above - they will produce predictions that deviate significantly from known values of y out of our training set for the same values of x. Minimizing the value of J(θ<sub>0</sub>,θ<sub>1</sub>) is our way of finding the best parameters for our hypothesis.\n",
    "\n",
    "### Why do we use squared errors as the cost function?\n",
    "Well squaring the errors should make sense, as it is a natural way in math to state a difference between values in absolute terms.\n",
    "There are other cost functions out there, and we'll learn about alternatives later, but given that this is a linear regression problem, squared errors are a pretty natural choice for our probelm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2\n",
    "\n",
    "Andrew starts the lecture simplifying the linear function by taking out the y-intercept parameter. This means that our hypothesis function will always cross through the origin of our plot.\n",
    "\n",
    "We can observe that the hypothesis is a function of x, which contains a term θ. The cost function is a function of θ.\n",
    "\n",
    "Consider a data set perfectly defined over y=x. If we input 1 as our definition of theta into our cost function, it should produce 0, which means that we've found a perfect hypothesis for fitting our data.\n",
    "\n",
    "When we plot values of our cost function over values of theta, we can see that a parabolic curve forms, with a global minima at theta = 1 (the cost is 0, because our hypothesis is also the function defining our data set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3\n",
    "\n",
    "The lecture includes contour plots and contour figures.\n",
    "\n",
    "We add back in the second theta parameter into our analysis, and when plotting our cost function in 3d space, we see a similar parabolic curve, but in a three dimensional bowl shape.\n",
    "\n",
    "You can use a three dimensional view, but another means of graphing is a contour plot, where concentric rings are plotted for assisting us visualize how close our parameters are to the global best choice.\n",
    "\n",
    "Conclusion: These plots are nice, but what we're after is an efficient program which can navigate these plots and automatically select the global minimum for our hypothesis parameters using the cost function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
