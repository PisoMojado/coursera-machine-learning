{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1\n",
    "In this lesson, Andrew discussed the Normal Equation in depth.\n",
    "\n",
    "Unlike gradient descent, which iteratively runs and converges on the global (or sometimes local) minimum, the normal equation would allow us to solve for the idea parameters $\\theta$ analytically (which I suspect means numerically).\n",
    "\n",
    "### Background to the Normal Equation\n",
    "\n",
    "How does the normal equation solve for the best set of parameters without the need for iterative calculations?\n",
    "\n",
    "Imagine a problem with only one feature, so $\\theta \\in \\mathbb{R}$\n",
    "Our cost function for our hypothesis is as follows:\n",
    "$$\\displaystyle J(\\theta) = a\\theta^2 + b\\theta + c$$\n",
    "This is a quadratic function, so how do you minimize it? In calculus, we do this by finding the derivative with respect to $\\theta$ and finding the value of $\\theta$ which produces 0 for the derivative function.\n",
    "\n",
    "Similarly, if we expand our problem with additional features, so $\\theta \\in \\mathbb{R}^{n+1}$, we expand our solution by performing partial derivatives for each parameter $\\theta_i$ for $\\theta_i \\in \\theta$. Once we have our system of partial derivatives, we simply solve for the values of $\\theta_i$ which result in 0 for our partial derivative functions.\n",
    "\n",
    "To quickly explain this to myself: this is because, when the slope of a function's derivative is 0, it means that the instantaneous rate of change at that point for our cost function is 0. Having no rate of change is an indicator in calculus that you have reached a local or global extremum; in linear regression, and in our quadratic equation earlier, we know that this is necessarily a global minimum, and so we have solved for the optimal $\\theta$ vector right away.\n",
    "\n",
    "### Normal Equation Example\n",
    "Andrew starts off by sharing a table of data (I pre-add the constant feature for brevity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant_feature =\n",
      "\n",
      "   1\n",
      "   1\n",
      "   1\n",
      "   1\n",
      "\n",
      "size_in_feet =\n",
      "\n",
      "   2104\n",
      "   1416\n",
      "   1534\n",
      "    852\n",
      "\n",
      "number_of_bedrooms =\n",
      "\n",
      "   5\n",
      "   3\n",
      "   3\n",
      "   2\n",
      "\n",
      "number_of_floors =\n",
      "\n",
      "   1\n",
      "   2\n",
      "   2\n",
      "   1\n",
      "\n",
      "age_of_home =\n",
      "\n",
      "   45\n",
      "   40\n",
      "   30\n",
      "   36\n",
      "\n",
      "price =\n",
      "\n",
      "   460\n",
      "   232\n",
      "   315\n",
      "   178\n",
      "\n",
      "training_set =\n",
      "\n",
      "      1   2104      5      1     45\n",
      "      1   1416      3      2     40\n",
      "      1   1534      3      2     30\n",
      "      1    852      2      1     36\n",
      "\n",
      "solution_set =\n",
      "\n",
      "      1   2104      5      1     45    460\n",
      "      1   1416      3      2     40    232\n",
      "      1   1534      3      2     30    315\n",
      "      1    852      2      1     36    178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "constant_feature = ones(4,1)\n",
    "size_in_feet = [2104;1416;1534;852]\n",
    "number_of_bedrooms = [5;3;3;2]\n",
    "number_of_floors = [1;2;2;1]\n",
    "age_of_home = [45;40;30;36]\n",
    "price = [460;232;315;178]\n",
    "\n",
    "training_set = [constant_feature, size_in_feet, number_of_bedrooms, number_of_floors, age_of_home]\n",
    "solution_set = [constant_feature, size_in_feet, number_of_bedrooms, number_of_floors, age_of_home, price]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrew refers to his training set matrix as $X$. He also defines his price vector to be $y$. You can see that $X$ is $m \\times (n+1)$ in dimensionality, and $y$ is $m \\times 1$ in dimensionality. Then, out of nowhere, Andrew just gives us the solution for the normal equation:\n",
    "$$\\theta = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "#### Observations\n",
    "\n",
    "Right off the bat, we see that we cannot multiply X and y together, because their \"adjacent\" dimensions are not the same size. We can do so, however, if we transpose $X$ to be $X^T$ they do line up.\n",
    "\n",
    "Why do we multiply this by X? Well, thinking about the results of this for a second, we will see every training event linearly combined (each dimension multiplied by the other event's, and all of it summed), with the $45^\\circ$ diagonal capturing a squared summation of each event. I wonder, does the inverse of the resulting matrix somehow capture the results from a  derivation?\n",
    "\n",
    "It's only a rough intuition, but it seems that, by manipulating our training data in the above formula, we're using the training data itself to extract the hypothesis parameters. This appears to be some consequence of our function being linear, and matrix operations.\n",
    "\n",
    "I'm going to pause this set of lessons, and instead I'm going to try find out [what this does](Least Squares.ipynb).\n",
    "\n",
    "#### Andrew's Explanation\n",
    "\n",
    "When givem $m$ examples, e.g. $(x^{(1)}, y^{(1)}), ... , (x^{(m)}, y^{(m)})$ and $n$ features, e.g.\n",
    "\n",
    "$$x^{(i)} = \\begin{bmatrix}x^{(i)}_0 \\\\ x^{(i)}_1 \\\\ \\vdots \\\\ x^{(i)}_n\\end{bmatrix} \\in \\mathbb{R}^{n+1}$$\n",
    "\n",
    "We define each training event as $x$, where x is a vector of the event's values for every feature (including the constant feature).\n",
    "\n",
    "We can construct the design matrix, $X$, by taking the transpose of each of these $x$, and stacking them onto each other:\n",
    "\n",
    "$$\\begin{bmatrix}--(x^{(1)})^T-- \\\\ --(x^{(2)})^T-- \\\\ \\vdots \\\\ --(x^{(m)})^T--\\end{bmatrix}$$\n",
    "\n",
    "### Personal thoughts\n",
    "\n",
    "So far, we have learned some basic techniques for applying matrix operations and intuitions from differential calculus to solving regression problems. This is all well and good, but so far it requires a human being to have some insight into the structure of the data, i.e. they need to know what proprties the hypothesis should have.\n",
    "\n",
    "There can be drawbacks to using the normal equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
