{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function Lecture Notes\n",
    "\n",
    "## Lecture 1\n",
    "\n",
    "In the first example, Andrew arranged a linear regression problem.\n",
    "The training set is a set of data that shows houses by their size in feet<sup>2</sup> as x, and their prices in 1m USD as y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = [[2104, 460], [1416, 232], [1534, 315], [852, 178]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider, then, that our training set can be viewed as a series of two-dimensional vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2104  460]\n",
      " [1416  232]\n",
      " [1534  315]\n",
      " [ 852  178]]\n"
     ]
    }
   ],
   "source": [
    "print(np.matrix(training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are seeking the predict the price of homes by their square footage, then we'd expect our hypothesis function to appear as a simple two-paramter linear function:\n",
    "\\begin{align}\n",
    "h_\\theta(x) & = \\theta_0 + \\theta_1x\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we want to choose (or discover) the values of θ<sub>0</sub> and θ<sub>1</sub> such that h<sub>θ</sub>(x) is close to the value of y in our training examples (x,y)\n",
    "\n",
    "Formally, we are going to find values for our parameters that minimize the value of J(θ<sub>0</sub>,θ<sub>1</sub>), where\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta_0,\\theta_1) & = \\frac{1}{2m}\\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right)^2\n",
    "\\end{align}\n",
    "Note: we are minimizing half of the squared difference because it makes the summation a little easier to work with later, and it is held as a given that minimizing by one half should yield the same ultimate values of θ<sub>0</sub> and θ<sub>1</sub>\n",
    "\n",
    "J(θ<sub>0</sub>,θ<sub>1</sub>) is what is called a **cost function**, and you can think of it as a way to measure the \"cost\" of choosing two particular parameters for your hypothesis function. Parameters which produce a (relatively) large value of J are bad, because - we can see this from how J is defined above - they will produce predictions that deviate significantly from known values of y out of our training set for the same values of x. Minimizing the value of J(θ<sub>0</sub>,θ<sub>1</sub>) is our way of finding the best parameters for our hypothesis.\n",
    "\n",
    "### Why do we use squared errors as the cost function?\n",
    "Well squaring the errors should make sense, as it is a natural way in math to state a difference between values in absolute terms.\n",
    "There are other cost functions out there, and we'll learn about alternatives later, but given that this is a linear regression problem, squared errors are a pretty natural choice for our probelm.\n",
    "\n",
    "## Lecture 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
